{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca1a287-35f6-4e74-999d-3534a5568222",
   "metadata": {},
   "source": [
    "# DQN implementation\n",
    "\n",
    "This notebook implements a DQN - an approximate q-learning algorithm with experience replay and target networks. Trains the algorithm on openAI's gym, to breakout Atari game, and monitors its games by exporting videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd137e-c1cd-485a-bc3b-9ca8cd48ddd3",
   "metadata": {},
   "source": [
    "For most problems, it is impractical to represent the $Q$-function as a table containing values for each combination of $s$ and $a$. Instead, we train a function approximator, such as a neural network with parameters $\\theta$, to estimate the Q-values, i.e. $Q(s, a; \\theta) \\approx Q^*(s, a)$. This can done by minimizing the following loss at each step $i$:\n",
    "\n",
    "$\\begin{equation}L_i(\\theta_i) = \\mathbb{E}_{s, a, r, s'\\sim \\rho(.)} \\left[ (y_i - Q(s, a; \\theta_i))^2 \\right]\\end{equation}$ where $y_i = r +  \\gamma \\max_{a'} Q(s', a'; \\theta_{i-1})$\n",
    "\n",
    "Here, $y_i$ is called the TD (temporal difference) target, and $y_i - Q$ is called the TD error.\n",
    "\n",
    "Note that the parameters from the previous iteration $\\theta_{i-1}$ are fixed and not updated. In practice we use a snapshot of the network parameters from a few iterations ago instead of the last iteration. This copy is called the *target network*.\n",
    "\n",
    "Q-Learning is an *off-policy* algorithm that learns about the greedy policy $a = \\max_{a} Q(s, a; \\theta)$ while using a different behaviour policy for acting in the environment/collecting data. This behaviour policy is usually an $\\epsilon$-greedy policy that selects the greedy action with probability $1-\\epsilon$ and a random action with probability $\\epsilon$ to ensure good coverage of the state-action space.\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "To avoid computing the full expectation in the DQN loss, we can minimize it using stochastic gradient descent. If the loss is computed using just the last transition $\\{s, a, r, s'\\}$, this reduces to standard Q-Learning. \n",
    "\n",
    "The Atari DQN work introduced a technique called Experience Replay to make the network updates more stable. At each time step of data collection, the transitions are added to a circular buffer called the *replay buffer*. Then during training, instead of using just the latest transition to compute the loss and its gradient, we compute them using a mini-batch of transitions sampled from the replay buffer. This has two advantages: better data efficiency by reusing each transition in many updates, and better stability using uncorrelated transitions in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724c6b7-e759-46a7-bc13-5b2b2ec819cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Input\n",
    "from keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df33ed-3f3f-4185-b33f-8e48ff3b80f8",
   "metadata": {},
   "source": [
    "Here is necessary constants, we will use them further during this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da21eb3-91f6-426f-b352-418880721ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZE = 84 # image size\n",
    "N_EPISODES = 4000 # total number of episodes\n",
    "N_STATE_FRAMES = 4 # number of used frames for developing state\n",
    "BATCH_SIZE = 64 # batch size\n",
    "MIN_BUFFER_SIZE = 16384 # minimal size of experience replay buffer, necessary count of experiences for starting training\n",
    "MAX_BUFFER_SIZE = 65536 # maximal size of experience replay buffer\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE_CYCLE = 10000 # number of steps, after which target model weights will be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780df06-5d8f-44fa-9232-cd44c5bc387a",
   "metadata": {},
   "source": [
    "In order to keep track of the data collected from the environment, we will use our own class ReplayBuffer. It stores experience data when we collect trajectories and is consumed during training.\n",
    "\n",
    "This replay buffer is constructed using specs describing the tensors that are to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf0e67-4ffd-43ce-ba43-a33e42eb1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self,\n",
    "                 max_size: int,\n",
    "                 batch_size: int,\n",
    "                 frame_width: int,\n",
    "                 frame_height: int,\n",
    "                 n_state_frames: int):\n",
    "        self.max_size = max_size\n",
    "        self.states = np.empty((self.max_size, frame_width, frame_height, n_state_frames), dtype=np.float32)\n",
    "        self.actions = np.empty(self.max_size, dtype=np.uint8)\n",
    "        self.rewards = np.empty(self.max_size, dtype=np.float32)\n",
    "        self.flags = np.empty(self.max_size, dtype=np.bool)\n",
    "        self.next_states = np.empty((self.max_size, frame_width, frame_height, n_state_frames), dtype=np.float32)\n",
    "        self.batch_size = batch_size\n",
    "        self.current_position = 0\n",
    "        self.current_experience = 0\n",
    "\n",
    "    def add_experience(self,\n",
    "                       state: np.ndarray,\n",
    "                       action: int,\n",
    "                       reward: float,\n",
    "                       done: bool,\n",
    "                       next_state: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add experience which was received from environment.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray with saved state. Current state.\n",
    "        action : int received from gym environment. Chosen action (randomly or from model).\n",
    "        reward : float, received from gym environment. Reward from current state.\n",
    "        done: bool, flag of terminal state, received from gym environment.\n",
    "        next_state: np.ndarray with saved state. Next state after performed action.\n",
    "        \"\"\"\n",
    "        self.states[self.current_position, ...] = state\n",
    "        self.actions[self.current_position] = action\n",
    "        self.rewards[self.current_position] = reward\n",
    "        self.flags[self.current_position] = done\n",
    "        self.next_states[self.current_position, ...] = next_state\n",
    "        self.current_position = (self.current_position + 1) % self.max_size\n",
    "        if self.current_experience < self.max_size:\n",
    "            self.current_experience += 1\n",
    "\n",
    "    def get_mini_batch(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \n",
    "        if self.current_experience < self.batch_size:\n",
    "            raise ValueError(f\"\"\"Not enough experience yet, required size = {self.batch_size}\"\"\")\n",
    "\n",
    "        indices = np.random.choice(np.arange(self.current_experience), self.batch_size, replace=False)\n",
    "\n",
    "        return (\n",
    "            self.states[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices],\n",
    "            self.next_states[indices],\n",
    "            self.flags[indices]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d140b54-a83d-46d2-bdf1-c76546ab0e53",
   "metadata": {},
   "source": [
    "## Deep Q-Network model\n",
    "\n",
    "### Evaluation networks\n",
    "\n",
    "We implemented light architecture of convolutional neural network. This model will be responsible for choosing particular actions, which depends on current state of the environment.\n",
    "\n",
    "\n",
    "### Target networks\n",
    "\n",
    "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
    "\n",
    "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
    "\n",
    "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
    "\n",
    "\n",
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb83da8-a403-4c4f-95e4-564b6cfd6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Model):\n",
    "    def __init__(self,\n",
    "                 n_actions: int,\n",
    "                 img_size: int,\n",
    "                 gamma: float,\n",
    "                 target_update_cycle: int,\n",
    "                 n_state_frames: int):\n",
    "        super(DQN, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.img_size = img_size\n",
    "        self.evaluation_network = self.build(self.img_size)\n",
    "        self.target_network = self.build(self.img_size)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "        self.gamma = gamma\n",
    "        self.loss_function = tf.keras.losses.Huber()\n",
    "        self.replay_buffer = ReplayBuffer(max_size=MAX_BUFFER_SIZE,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          n_state_frames=n_state_frames,\n",
    "                                          frame_width=self.img_size,\n",
    "                                          frame_height=self.img_size)\n",
    "        self.target_update_counter = 0\n",
    "        self.target_update_cycle = target_update_cycle\n",
    "\n",
    "    def build(self, img_size):\n",
    "        dqn_model = Sequential(\n",
    "            [\n",
    "                Conv2D(32, 8, 8, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "                Activation('relu'),\n",
    "                Conv2D(64, 4, 4, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "                Activation('relu'),\n",
    "                Conv2D(128, 2, 2, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "                Activation('relu'),\n",
    "                Flatten(),\n",
    "                Dense(512, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2)),\n",
    "                Activation('relu'),\n",
    "                Dense(self.n_actions, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2))\n",
    "            ]\n",
    "        )\n",
    "        return dqn_model\n",
    "\n",
    "    def train(self):\n",
    "        states, rewards, actions, next_states, flags = self.replay_buffer.get_mini_batch()\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_eval_arr = self.evaluation_network(states)\n",
    "            q_eval = tf.reduce_max(q_eval_arr, axis=1)\n",
    "            target_q_values = self.target_network(next_states)\n",
    "            discount_factor = tf.reduce_max(target_q_values)\n",
    "            q_target = rewards + self.gamma * discount_factor\n",
    "            loss = tf.reduce_mean(self.loss_function(q_eval, q_target))\n",
    "\n",
    "        gradients = tape.gradient(loss, self.evaluation_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.evaluation_network.trainable_variables))\n",
    "        self.target_update_counter += 1\n",
    "\n",
    "        if self.target_update_counter % self.target_update_cycle == 0:\n",
    "            self.target_network.set_weights(self.evaluation_network.get_weights())\n",
    "\n",
    "    def add_experience(self, state, action, reward, done, next_state):\n",
    "        self.replay_buffer.add_experience(state, action, reward, done, next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa24a2-5976-4842-be74-4a0c1f0210c7",
   "metadata": {},
   "source": [
    "### Processing game image \n",
    "\n",
    "Raw atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn them.\n",
    "\n",
    "We can thus save a lot of time by preprocessing game image, including\n",
    "* Resizing to a smaller shape, 80 x 80\n",
    "* Converting to grayscale\n",
    "* Cropping irrelevant image parts (top & bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea1d00-1663-43bd-9d43-2389e6830ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformer:\n",
    "    def __init__(self, im_size, method):\n",
    "        self.im_size = im_size\n",
    "        self.method = method\n",
    "\n",
    "    def transform(self, image) -> tf.Tensor:\n",
    "        output = tf.image.rgb_to_grayscale(image)\n",
    "        output = tf.image.crop_to_bounding_box(output, 34, 0, 160, 160)\n",
    "        output = tf.image.resize(output,\n",
    "                                 [self.im_size, self.im_size],\n",
    "                                 method=self.method)\n",
    "        output = tf.squeeze(output)\n",
    "        output = output.numpy() / 255.0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93648d39-356c-4596-9e4a-dbbff7ce31ff",
   "metadata": {},
   "source": [
    "Simple function for updating state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b938b88-4d90-4440-9e15-9c858333e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(state, obs_small):\n",
    "    return np.append(state[:, :, 1:], np.expand_dims(obs_small, 2), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5216fb-a3be-474e-acd3-fd892f8630db",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277fbfc-5ec1-4985-b431-d5e01ea119b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_minimal_experience(model: DQN,\n",
    "                            img_transformer: ImageTransformer):\n",
    "    env = gym.make('Breakout-v0')\n",
    "    current_experience = 0\n",
    "    obs = env.reset()\n",
    "    small_obs = img_transformer.transform(obs)\n",
    "    state = np.stack([small_obs] * 4, axis=2)\n",
    "    done = False\n",
    "    while current_experience < MIN_BUFFER_SIZE:\n",
    "        action = env.action_space.sample()\n",
    "        current_experience += 1\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs_small = img_transformer.transform(obs)\n",
    "        next_state = update_state(state, obs_small)\n",
    "        model.add_experience(state, action, reward, done, next_state)\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            small_obs = img_transformer.transform(obs)\n",
    "            state = np.stack([small_obs] * 4, axis=2)\n",
    "    env.close()\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4eccb4-fc5f-429e-9698-58e199c8a5dd",
   "metadata": {},
   "source": [
    "And we are ready to learn & play. We will learn our model through episodes with our experience replay buffer, make random sample from buffer every time, and learn model from this batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94395393-9d3e-407d-8b35-7344b78ccc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    env = gym.make('Breakout-v0')\n",
    "    N_ACTIONS = env.action_space.n\n",
    "    INITIAL_EPSILON = 0.4\n",
    "    FINAL_EPSILON = 0.01\n",
    "    EPSILON_DECAY = 1000000\n",
    "    TRAINING_CYCLE = 2000\n",
    "    epsilon = INITIAL_EPSILON\n",
    "\n",
    "    # outdir = './results'\n",
    "    # env = gym.wrappers.Monitor(env, directory=outdir, force=True)\n",
    "    network = DQN(n_actions=N_ACTIONS,\n",
    "                  img_size=IM_SIZE,\n",
    "                  gamma=GAMMA,\n",
    "                  n_state_frames=N_STATE_FRAMES,\n",
    "                  target_update_cycle=TARGET_UPDATE_CYCLE)\n",
    "    img_transformer = ImageTransformer(im_size=IM_SIZE,\n",
    "                                       method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # gaining minimal experience\n",
    "    network = gain_minimal_experience(model=network,\n",
    "                                      img_transformer=img_transformer)\n",
    "    for episode in range(N_EPISODES):\n",
    "        if epsilon > FINAL_EPSILON:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EPSILON_DECAY\n",
    "\n",
    "        episode_reward = 0\n",
    "        obs = env.reset()\n",
    "        small_obs = img_transformer.transform(obs)\n",
    "        state = np.stack([small_obs] * 4, axis=2)\n",
    "        t = 0\n",
    "        while True:\n",
    "            # env.render()\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                state = np.expand_dims(state, axis=0)\n",
    "                tmp = network.evaluation_network(state)\n",
    "                action = tf.argmax(tmp[0])\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            small_obs = img_transformer.transform(obs)\n",
    "            state = np.squeeze(state)\n",
    "            next_state = update_state(state, small_obs)\n",
    "            network.add_experience(state, action, reward, done, next_state)\n",
    "            episode_reward += reward\n",
    "\n",
    "            network.train()\n",
    "            state = next_state\n",
    "            if done:\n",
    "                logging.info(\n",
    "                    'Episode {} finished after {} timesteps, total rewards {}'.format(episode, t + 1, episode_reward))\n",
    "                break\n",
    "            t += 1\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
